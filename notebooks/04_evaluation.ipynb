{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluations\n",
    "Runs through the different stages of the RAG agent and evaluates the performance of the different stages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "sys.path.append(str(Path.cwd().parent))\n",
    "\n",
    "LIMIT = 100 # set the number of questions to evaluate\n",
    "results = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data and prep the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "data_dir = os.path.join('..', 'ConvFinQA/data/')\n",
    "\n",
    "train_data = json.load(open(os.path.join(data_dir, 'train.json')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not all the data has qa, so we need to filter out the data that doesn't have qa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_data = [\n",
    "    {\n",
    "        'id': data['id'],\n",
    "        'question': data[qa_key]['question'],\n",
    "        'answer': data[qa_key]['answer']\n",
    "    }\n",
    "    for data in train_data\n",
    "    for qa_key in [k for k in data.keys() if k == 'qa' or k.startswith('qa_')] # account for multiple qa keys\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3037\n",
      "3965\n"
     ]
    }
   ],
   "source": [
    "print(len(train_data))\n",
    "print(len(qa_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'id': 'Single_JKHY/2009/page_28.pdf-3',\n",
       "  'question': 'what was the percentage change in the net cash from operating activities from 2008 to 2009',\n",
       "  'answer': '14.1%'},\n",
       " {'id': 'Single_RSG/2008/page_114.pdf-2',\n",
       "  'question': 'what was the percent of the growth in the revenues from 2007 to 2008',\n",
       "  'answer': '1.3%'},\n",
       " {'id': 'Single_AAPL/2002/page_23.pdf-1',\n",
       "  'question': 'what was the percentage change in net sales from 2000 to 2001?',\n",
       "  'answer': '-32%'},\n",
       " {'id': 'Single_UPS/2009/page_33.pdf-2',\n",
       "  'question': 'what was the difference in percentage cumulative return on investment for united parcel service inc . compared to the s&p 500 index for the five year period ended 12/31/09?',\n",
       "  'answer': '-26.16%'},\n",
       " {'id': 'Double_UPS/2009/page_33.pdf',\n",
       "  'question': 'what is the roi of an investment in ups in 2004 and sold in 2006?',\n",
       "  'answer': '-8.9%'}]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa_data[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only take the first LIMIT questions\n",
    "limit_qa_data = qa_data[:LIMIT]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieval\n",
    "First let's take a look at the retrieval stage.\n",
    "\n",
    "For this I will look at Recall and Mean Reciprocal Rank (MRR). \n",
    "\n",
    "Precision is less relevant here as we always retrieve K documents and there's only 1 relevant document for each question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.nodes import retriever_node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for data in limit_qa_data:\n",
    "    input_dict = {'question': data['question']}\n",
    "    retrieved_docs = retriever_node(input_dict)['retrieved_docs']\n",
    "    data['retrieved_docs'] = retrieved_docs\n",
    "\n",
    "    retrieved_docs_ids = [doc['id'] for doc in retrieved_docs]\n",
    "    if data['id'] in retrieved_docs_ids:\n",
    "        data['retrieved'] = 1\n",
    "        rank = retrieved_docs_ids.index(data['id']) + 1  # +1 because index starts at 0\n",
    "        data['reciprocal_rank'] = 1.0 / rank\n",
    "    else:\n",
    "        data['retrieved'] = 0\n",
    "        data['reciprocal_rank'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall Retrieval Recall: 0.620\n",
      "Overall Retrieval MRR: 0.268\n",
      "Successful Retrieval MRR: 0.432\n"
     ]
    }
   ],
   "source": [
    "# Calculate metrics\n",
    "retrieved_data = [d for d in limit_qa_data if d['retrieved'] == 1]\n",
    "num_retrieved = len(retrieved_data)\n",
    "\n",
    "retrieval_recall = sum(data['retrieved'] for data in limit_qa_data) / len(limit_qa_data)\n",
    "retrieval_mrr = sum(data['reciprocal_rank'] for data in limit_qa_data) / len(limit_qa_data)\n",
    "retrieval_mrr_retrieved = sum(data['reciprocal_rank'] for data in retrieved_data) / num_retrieved\n",
    "\n",
    "print(f\"Overall Retrieval Recall: {retrieval_recall:.3f}\")\n",
    "print(f\"Overall Retrieval MRR: {retrieval_mrr:.3f}\")\n",
    "print(f\"Successful Retrieval MRR: {retrieval_mrr_retrieved:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "results['overall_retrieval_recall'] = retrieval_recall\n",
    "results['overall_retrieval_mrr'] = retrieval_mrr\n",
    "results['successful_retrieval_mrr'] = retrieval_mrr_retrieved"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reranking\n",
    "Now let's analyse the reranking.\n",
    "\n",
    "This will also use Recall and MRR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.nodes import reranker_node\n",
    "\n",
    "import time\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reranking Documents: 100%|██████████| 100/100 [10:41<00:00,  6.41s/it]\n"
     ]
    }
   ],
   "source": [
    "for data in tqdm(limit_qa_data, desc=\"Reranking Documents\"):\n",
    "    input_dict = {'question': data['question'], 'retrieved_docs': data['retrieved_docs']}\n",
    "    reranked_docs = reranker_node(input_dict)['reranked_docs']\n",
    "    data['reranked_docs'] = reranked_docs\n",
    "\n",
    "    reranked_docs_ids = [doc['id'] for doc in reranked_docs]\n",
    "    if data['id'] in reranked_docs_ids:\n",
    "        data['reranked'] = 1\n",
    "        rank = reranked_docs_ids.index(data['id']) + 1  # +1 because index starts at 0\n",
    "        data['reranked_reciprocal_rank'] = 1.0 / rank\n",
    "    else:\n",
    "        data['reranked'] = 0\n",
    "        data['reranked_reciprocal_rank'] = 0\n",
    "\n",
    "    # Using the free tier Cohere API you get 10 requests per minute\n",
    "    # Comment this if you have a paid tier\n",
    "    time.sleep(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall Reranking Recall: 0.540\n",
      "Overall Reranking MRR: 0.427\n",
      "Successful Reranking Recall: 0.871\n",
      "Successful Reranking MRR: 0.790\n"
     ]
    }
   ],
   "source": [
    "# Calculate metrics\n",
    "reranked_data = [d for d in limit_qa_data if d['reranked'] == 1]\n",
    "num_reranked = len(reranked_data)\n",
    "\n",
    "reranking_recall = sum(data['reranked'] for data in limit_qa_data) / len(limit_qa_data)\n",
    "reranking_mrr = sum(data['reranked_reciprocal_rank'] for data in limit_qa_data) / len(limit_qa_data)\n",
    "reranked_recall_retrieved = sum(data['reranked'] for data in retrieved_data) / num_retrieved\n",
    "reranking_mrr_retrieved = sum(data['reranked_reciprocal_rank'] for data in reranked_data) / num_reranked\n",
    "\n",
    "print(f\"Overall Reranking Recall: {reranking_recall:.3f}\")\n",
    "print(f\"Overall Reranking MRR: {reranking_mrr:.3f}\")\n",
    "print(f\"Successful Reranking Recall: {reranked_recall_retrieved:.3f}\")\n",
    "print(f\"Successful Reranking MRR: {reranking_mrr_retrieved:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = 'limit_qa_data.json'\n",
    "with open(output_path, 'w') as f:\n",
    "    json.dump(limit_qa_data, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for tomorrow:\n",
    "# 1. check over previous work and make sure it makes sense\n",
    "# 2. run the LLM corectness test, both with and without retrieval_mrr\n",
    "# 3. Add metadta filtering and test effect on retrieval \n",
    "# 4. Write report\n",
    "# 5. Update readme"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLM Correctness\n",
    "In other RAG systems where the answer is expected as a paragraph or may use multiple sources, LLM faithfulness can be  agood metric. In our case with ConvFinQA, the answer is expected as a single number, and therefore a more binary metric is more appropriate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14.1%\n",
      "1.3%\n",
      "-32%\n",
      "-26.16%\n",
      "-8.9%\n",
      "-26.16%\n",
      "70.1%\n",
      "15.6%\n",
      "15.7%\n",
      "16%\n",
      "22.99%\n",
      "12\n",
      "3\n",
      "-19\n",
      "2.4%\n",
      "56.6%\n",
      "2.6\n",
      "11%\n",
      "158.82%\n",
      "11.1%\n",
      "16.7%\n",
      "14.7%\n",
      ".43\n",
      "11.2%\n",
      "40.3\n",
      "594840\n",
      "700%\n",
      "30%\n",
      "-1.2%\n",
      "350824 thousand\n",
      "60.3\n",
      "165%\n",
      "67%\n",
      "83.6%\n",
      "13.25%\n",
      "-57%\n",
      "7020\n",
      "-2%\n",
      "377000\n",
      "31.14%\n",
      "-13.4%\n",
      "1.79\n",
      "-31.7%\n",
      "18%\n",
      "98.2%\n",
      "$ 110774.5 million\n",
      "1.5%\n",
      "-489.2\n",
      "6.1%\n",
      "5805209\n",
      "31.7%\n",
      "15.16%\n",
      "11.55%\n",
      "32%\n",
      "24.4%\n",
      "-76.8%\n",
      "7.61%\n",
      "17%\n",
      "-16.6%\n",
      "-14.9%\n",
      "49%\n",
      "3085000\n",
      "22.86%\n",
      "47.4%\n",
      "-11.7%\n",
      "-1281\n",
      "69%\n",
      "1905.4\n",
      "67%\n",
      "45.3%\n",
      "58%\n",
      "5%\n",
      "21%\n",
      "8.1%\n",
      "359.67%\n",
      "-6.4%\n",
      "645\n",
      "89.14%\n",
      "808.5%\n",
      "45%\n",
      "25.4%\n",
      "9.9%\n",
      "12950000\n",
      "39.9%\n",
      "-34.4%\n",
      "-11.3%\n",
      "-78.8%\n",
      "2.3%\n",
      "25%\n",
      "7%\n",
      "3.1%\n",
      "14.2%\n",
      "64.6%\n",
      "2484034\n",
      "3%\n",
      "97.8%\n",
      "66%\n",
      "48%\n",
      "84%\n",
      "17%\n"
     ]
    }
   ],
   "source": [
    "for data in limit_qa_data:\n",
    "    print(data['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
